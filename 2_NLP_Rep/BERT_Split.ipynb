{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1901d550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "CHUNK_SIZE = 256\n",
    "EPOCHS = 3\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b2cb801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def pre_process(data):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=0,\n",
    "        length_function=lambda x: len(tokenizer.encode(x, add_special_tokens=True))\n",
    "    )\n",
    "    pairs = [(chunk, row[0]) for row in tqdm(data[[\"cord_uid\", \"abstract\"]].to_numpy(), desc=\"Creating pairs\") for chunk in splitter.split_text(row[1])]\n",
    "\n",
    "    docs = [Document(page_content=pair[0], metadata={\"cord_uid\": pair[1]}) for pair in pairs]\n",
    "    titles = [Document(page_content=row[1], metadata={\"cord_uid\": row[0]}) for row in data[[\"cord_uid\", \"title\"]].to_numpy()]\n",
    "\n",
    "    docs.extend(titles)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b97f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Preprocessing step.\n",
    "# If docs.json already exists, skip this step.\n",
    "\n",
    "with open(\"../X_Data/subtask4b_collection_data.pkl\", \"rb\") as f:\n",
    "    data = pkl.load(f)\n",
    "    collection = pd.DataFrame(data)\n",
    "\n",
    "docs = pre_process(collection)\n",
    "\n",
    "for doc in docs:\n",
    "    if len(tokenizer.encode(doc.page_content, add_special_tokens=True)) > CHUNK_SIZE:\n",
    "        print(f\"Document {doc.metadata['cord_uid']} is too long: {len(doc.page_content)} characters\")\n",
    "        raise Exception(\"Doc too long\")\n",
    "\n",
    "objs = [\n",
    "    {\n",
    "        \"cord_uid\": doc.metadata[\"cord_uid\"],\n",
    "        \"text\": doc.page_content,\n",
    "    } for doc in docs\n",
    "]\n",
    "\n",
    "with open(\"docs.json\", \"w\") as f:\n",
    "    json.dump(objs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8518f95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"docs.json\", \"r\") as f:\n",
    "    paper_chunks = json.load(f)\n",
    "\n",
    "# Group chunks by paper ID\n",
    "paper_dict = {}\n",
    "for entry in paper_chunks:\n",
    "    uid = entry[\"cord_uid\"]\n",
    "    paper_dict.setdefault(uid, []).append(entry[\"text\"])\n",
    "\n",
    "df = pd.read_csv(\"../X_Data/subtask4b_query_tweets_train.tsv\", sep=\"\\t\")\n",
    "train_df = df[[\"tweet_text\", \"cord_uid\"]].dropna()\n",
    "dev_df = pd.read_csv(\"../X_Data/subtask4b_query_tweets_dev.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class TweetPaperDataset(Dataset):\n",
    "    def __init__(self, df, paper_dict, tokenizer, max_len=256, num_negatives=1):\n",
    "        self.df = df\n",
    "        self.paper_dict = paper_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.neg = num_negatives\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        tweet = row[\"tweet_text\"]\n",
    "        pos_id = row[\"cord_uid\"]\n",
    "\n",
    "        pos_chunks = self.paper_dict.get(pos_id, [])\n",
    "        pos_text = random.choice(pos_chunks) if pos_chunks else \"\"\n",
    "\n",
    "        all_ids = list(self.paper_dict.keys())\n",
    "        all_ids.remove(pos_id)\n",
    "        neg_ids = random.sample(all_ids, self.neg)\n",
    "        neg_chunks = [random.choice(self.paper_dict[nid]) for nid in neg_ids]\n",
    "\n",
    "        tweet_inputs = self.tokenizer(tweet, padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "        pos_inputs = self.tokenizer(pos_text, padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "        neg_inputs = [self.tokenizer(chunk, padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors=\"pt\") for chunk in neg_chunks]\n",
    "\n",
    "        return {\n",
    "            \"tweet\": tweet_inputs,\n",
    "            \"pos\": pos_inputs,\n",
    "            \"neg\": neg_inputs\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualBertEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859aa73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(tweet_emb, pos_emb, neg_embs):\n",
    "    # Triplet loss: bring tweet closer to positive than negative\n",
    "    pos_sim = nn.functional.cosine_similarity(tweet_emb, pos_emb)\n",
    "    loss = 0\n",
    "    for neg in neg_embs:\n",
    "        neg_sim = nn.functional.cosine_similarity(tweet_emb, neg)\n",
    "        loss += torch.mean(nn.functional.relu(1 - pos_sim + neg_sim))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, epoch=0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=f\"Training epoch {epoch}\"):\n",
    "        tweet_inputs = batch[\"tweet\"]\n",
    "        pos_inputs = batch[\"pos\"]\n",
    "        neg_inputs = batch[\"neg\"]\n",
    "\n",
    "        tweet_emb = model(tweet_inputs[\"input_ids\"].squeeze(1).to(device),\n",
    "                          tweet_inputs[\"attention_mask\"].squeeze(1).to(device))\n",
    "        pos_emb = model(pos_inputs[\"input_ids\"].squeeze(1).to(device),\n",
    "                        pos_inputs[\"attention_mask\"].squeeze(1).to(device))\n",
    "        neg_embs = [model(n[\"input_ids\"].squeeze(1).to(device),\n",
    "                          n[\"attention_mask\"].squeeze(1).to(device)) for n in neg_inputs]\n",
    "\n",
    "        loss = compute_loss(tweet_emb, pos_emb, neg_embs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c415dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3.42131-fa1d09cbd\n"
     ]
    }
   ],
   "source": [
    "from os import putenv\n",
    "\n",
    "putenv(\"HSA_OVERRIDE_GFX_VERSION\", \"11.0.0\")\n",
    "putenv(\"PYTORCH_ROCM_ARCH\", \"gfx1100\")\n",
    "putenv(\"HIP_VISIBLE_DEVICES\", \"0\")\n",
    "putenv(\"ROOCM_PATH\", \"/opt/rocm-6.3.4\")\n",
    "putenv(\"HIP_PLATFORM\", \"amd\")\n",
    "putenv(\"HIP_DEVICE\", \"0\")\n",
    "putenv(\"AMD_SERIALIZE_KERNEL\", \"3\")\n",
    "putenv(\"AMD_LOG_LEVEL\", \"5\")\n",
    "\n",
    "print(torch.version.hip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9630da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TweetPaperDataset(train_df, paper_dict, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "model = DualBertEncoder().to(device)\n",
    "if os.path.exists(\"bert_dual_encoder.pt\"):\n",
    "    model.load_state_dict(torch.load(\"bert_dual_encoder.pt\"))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "if os.path.exists(\"bert_dual_encoder_optimizer.pt\"):\n",
    "    optimizer.load_state_dict(torch.load(\"bert_dual_encoder_optimizer.pt\"))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    loss = train(model, train_loader, optimizer, epoch=epoch)\n",
    "    print(f\"Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bc92969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bert_dual_encoder.pt\")\n",
    "torch.save(optimizer.state_dict(), \"bert_dual_encoder_optimizer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dabc554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_texts(texts, model, tokenizer, max_len=256, batch_size=32):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        encodings = tokenizer(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(encodings[\"input_ids\"], encodings[\"attention_mask\"])\n",
    "        embeddings.extend(output.cpu())\n",
    "    return torch.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c752ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_chunk_texts = []\n",
    "paper_chunk_uids = []\n",
    "\n",
    "for uid, chunks in paper_dict.items():\n",
    "    for chunk in chunks:\n",
    "        paper_chunk_texts.append(chunk)\n",
    "        paper_chunk_uids.append(uid)\n",
    "\n",
    "# paper_chunk_embeddings = embed_texts(paper_chunk_texts, model, tokenizer)\n",
    "# print(f\"Embedded {len(paper_chunk_texts)} paper chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72de8949",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_tweets = dev_df[\"tweet_text\"].tolist()\n",
    "dev_ids = dev_df[\"cord_uid\"].tolist()\n",
    "\n",
    "# tweet_embeddings = embed_texts(dev_tweets, model, tokenizer)\n",
    "# print(f\"Embedded {len(dev_tweets)} dev tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c6d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"embeddings\": paper_chunk_embeddings,\n",
    "    \"uids\": paper_chunk_uids\n",
    "}, \"paper_chunk_embeddings.pt\")\n",
    "\n",
    "torch.save({\n",
    "    \"embeddings\": tweet_embeddings,\n",
    "    \"tweets\": dev_tweets,\n",
    "    \"cord_uids\": dev_ids\n",
    "}, \"dev_tweet_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0186ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tweets: 1400it [18:21,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweet: covid recovery: this study from the usa reveals that a proportion of cases experience impairment in some cognitive functions for several months after infection. some possible biases &amp; limitations but more research is required on impact of these long term effects.\n",
      "Gold paper: 3qvh482o\n",
      "Top 5 retrieved: ['yf3z913h', 'ws75gpsc', 'lj37a4xn', 'n2rec4i8', 'ho6qjkyr']\n",
      "\n",
      "Tweet: \"Among 139 clients exposed to two symptomatic hair stylists with confirmed COVID-19, while both the stylists & the clients wore face masks, no additional symptomatic cases were reported; among 67 clients tested for SARS-CoV-2, all test results were negative\"\n",
      "Gold paper: r58aohnu\n",
      "Top 5 retrieved: ['r58aohnu', 'astxi4el', '2e02uktc', 'h3tor88n', 'ydv0hc0m']\n",
      "\n",
      "Tweet: I recall early on reading that researchers who had examined other coronaviruses discovered that individuals could contract the same virus multiple times within the same year.  I even located a source for it!\n",
      "Gold paper: sts48u9i\n",
      "Top 5 retrieved: ['r67lnbfc', 'xr70oiv3', 'c4h0x85r', '11os51qg', 'od0y5ruh']\n",
      "\n",
      "Tweet: You know you're credible when NIH website has your paper ðŸ’ƒðŸ’ƒ someone is paying attention to data. Missing microbes of COVID-19: Bifidobacterium, Faecalibacterium depletion and decreased microbiome diversity associated with SARS-CoV-2 infection severity - PubMed\n",
      "Gold paper: 3sr2exq9\n",
      "Top 5 retrieved: ['3sr2exq9', 'o0yvgmgh', '89r29l8g', 'lzddnb8j', 'ant6397h']\n",
      "\n",
      "Tweet: Resistance to antifungal medications is a growing issue globally in both scope and duration, encompassing novel resistant strains of previously susceptible pathogens as well as entirely new emerging species that are resistant to multiple antifungal treatments #amr\n",
      "Gold paper: ybwwmyqy\n",
      "Top 5 retrieved: ['0cam9ipf', 'lzddnb8j', '4j8b8z4t', 'ttytoz3v', 'tded20ih']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "top_k = 5\n",
    "results = []\n",
    "\n",
    "for idx, tweet_emb in tqdm(enumerate(tweet_embeddings), desc=\"Tweets\"):\n",
    "    scores = defaultdict(float)\n",
    "    \n",
    "    for chunk_idx, chunk_emb in tqdm(enumerate(paper_chunk_embeddings), desc=\"Chunks\", leave=False):\n",
    "        uid = paper_chunk_uids[chunk_idx]\n",
    "        sim = cosine_similarity(tweet_emb.unsqueeze(0), chunk_emb.unsqueeze(0)).item()\n",
    "        scores[uid] = max(scores[uid], sim)  # Keep best similarity per paper\n",
    "\n",
    "    # Sort by similarity and get top_k papers\n",
    "    top_papers = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    results.append({\n",
    "        \"tweet\": dev_tweets[idx],\n",
    "        \"gold_cord_uid\": dev_ids[idx],\n",
    "        \"retrieved\": [uid for uid, score in top_papers]\n",
    "    })\n",
    "\n",
    "# Display a sample\n",
    "for r in results[5:10]:\n",
    "    print(f\"\\nTweet: {r['tweet']}\")\n",
    "    print(f\"Gold paper: {r['gold_cord_uid']}\")\n",
    "    print(\"Top 5 retrieved:\", r[\"retrieved\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00a6e7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweet: IL-6 seems to be a primary catalyst of this uncontrolled inflammation in #covid19, and #tocilizumab, a mab IL-6 receptor blocker, has been used in small series of severe covid-19 cases with early reports of success\n",
      "Gold paper: 8cvjsisw\n",
      "Top 5 retrieved: ['ysd9pmq1', 'et1ekgdl', '36zu137v', '3bo4md44', '0b1dbz6q']\n",
      "\n",
      "Tweet: macro-level, multi-national analysis shows that public mask compliance is negligible for stopping the spread of covid-19  â€” it appears to be socially damaging.\n",
      "Gold paper: tra5ewc5\n",
      "Top 5 retrieved: ['f96qs295', '9b6cepf4', 'nv1w6juh', 'zl4ixyg1', 'opjfy3xr']\n",
      "\n",
      "Tweet: Long, slender transmission chains of severe acute respiratory syndrome coronavirus (sars-cov-2) may go undetected for several weeks at low to moderate reproductive numbers: implications for containment and elimination strategy  [ðŸš¨preprint]\n",
      "Gold paper: yoiq6cgt\n",
      "Top 5 retrieved: ['yoiq6cgt', '3o5c0l24', 'x0cs571f', 'yjbmi8ur', 'wwt7mn55']\n",
      "\n",
      "Tweet: Significant vitamin D deficiency in people with COVID-19 was associated with a substantially higher mortality risk than COVID-19 patients with typical vitamin D levels (retrospective study in Bari, Italy).\n",
      "Gold paper: be8eu3qi\n",
      "Top 5 retrieved: ['y0mbzzx0', 'eakfj0wv', 'z2jtzsl6', 'c25womxb', 'xmtv0v6q']\n",
      "\n",
      "Tweet: \"A strong connection is found between low serum zinc and serious cases of covid-19.\"\n",
      "Gold paper: c0ipfbeg\n",
      "Top 5 retrieved: ['qpnjloe6', 'omrb959r', 'ypp1foym', 'ho832o9g', 'b0qj7tck']\n",
      "Mean Reciprocal Rank (MRR@5): 0.1524\n"
     ]
    }
   ],
   "source": [
    "for r in results[5:10]:\n",
    "    print(f\"\\nTweet: {r['tweet']}\")\n",
    "    print(f\"Gold paper: {r['gold_cord_uid']}\")\n",
    "    print(\"Top 5 retrieved:\", r[\"retrieved\"])\n",
    "\n",
    "def compute_mrr5(results):\n",
    "    mrr = 0\n",
    "    for result in results:\n",
    "        gold_uid = result[\"gold_cord_uid\"]\n",
    "        retrieved = result[\"retrieved\"]\n",
    "        if gold_uid in retrieved:\n",
    "            rank = retrieved.index(gold_uid) + 1\n",
    "            mrr += 1 / rank\n",
    "    return mrr / len(results)\n",
    "\n",
    "mrr = compute_mrr5(results)\n",
    "print(f\"Mean Reciprocal Rank (MRR@5): {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be48a42b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m openai \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m EMBEDDING_BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_openai_embeddings\u001b[39m(texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], sleep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_client.py:116\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    114\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "# Compare to performance with OpenAI embeddings\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "EMBEDDING_BATCH_SIZE = 1000 \n",
    "\n",
    "def get_openai_embeddings(texts: list[str], sleep=False):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), EMBEDDING_BATCH_SIZE)):\n",
    "        batch = texts[i:i+EMBEDDING_BATCH_SIZE]\n",
    "        batch = [t.replace(\"\\n\", \"\").strip() for t in batch]\n",
    "        print(len(batch))\n",
    "\n",
    "        embedding_response = openai.embeddings.create(\n",
    "            input=batch,\n",
    "            model=\"text-embedding-3-large\"\n",
    "        )\n",
    "        print(\"Embeddings\")\n",
    "        embeddings.extend([np.array(embedding.embedding) for embedding in embedding_response.data])\n",
    "        if sleep:\n",
    "            time.sleep(3)\n",
    "    return embeddings\n",
    "\n",
    "openai_tweet_embeddings = get_openai_embeddings(dev_tweets)\n",
    "\n",
    "openai_paper_chunk_embeddings = get_openai_embeddings(paper_chunk_texts, sleep=True)\n",
    "\n",
    "torch_tweet_embeddings = [torch.from_numpy(a) for a in openai_tweet_embeddings]\n",
    "torch_paper_chunk_embeddings = [torch.from_numpy(a) for a in openai_paper_chunk_embeddings]\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "similarities = []\n",
    "for tweet_emb in tqdm(openai_tweet_embeddings, desc=\"Tweets\"):\n",
    "    scores = defaultdict(float)\n",
    "    \n",
    "    for chunk_emb in tqdm(openai_paper_chunk_embeddings, desc=\"Chunks\", leave=False):\n",
    "        sim = cosine_similarity(tweet_emb.unsqueeze(0), chunk_emb.unsqueeze(0)).item()\n",
    "        scores[uid] = max(scores[uid], sim)  # Keep best similarity per paper\n",
    "\n",
    "    # Sort by similarity and get top_k papers\n",
    "    top_papers = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    similarities.append({\n",
    "        \"tweet\": dev_tweets[idx],\n",
    "        \"gold_cord_uid\": dev_ids[idx],\n",
    "        \"retrieved\": [uid for uid, score in top_papers]\n",
    "    })\n",
    "\n",
    "# Display a sample\n",
    "for r in similarities[5:10]:\n",
    "    print(f\"\\nTweet: {r['tweet']}\")\n",
    "    print(f\"Gold paper: {r['gold_cord_uid']}\")\n",
    "    print(\"Top 5 retrieved:\", r[\"retrieved\"])\n",
    "\n",
    "def compute_mrr5(results):\n",
    "    mrr = 0\n",
    "    for result in results:\n",
    "        gold_uid = result[\"gold_cord_uid\"]\n",
    "        retrieved = result[\"retrieved\"]\n",
    "        if gold_uid in retrieved:\n",
    "            rank = retrieved.index(gold_uid) + 1\n",
    "            mrr += 1 / rank\n",
    "    return mrr / len(results)\n",
    "mrr = compute_mrr5(similarities)\n",
    "print(f\"Mean Reciprocal Rank (MRR@5): {mrr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1756ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tweet_embeddings = [torch.from_numpy(a) for a in openai_tweet_embeddings]\n",
    "torch_paper_chunk_embeddings = [torch.from_numpy(a) for a in openai_paper_chunk_embeddings]\n",
    "\n",
    "torch.save({\n",
    "    \"embeddings\": torch_paper_chunk_embeddings,\n",
    "    \"uids\": paper_chunk_uids\n",
    "}, \"paper_chunk_embeddings.pt\")\n",
    "torch.save({\n",
    "    \"embeddings\": torch_tweet_embeddings,\n",
    "    \"tweets\": dev_tweets,\n",
    "    \"cord_uids\": dev_ids\n",
    "}, \"dev_tweet_embeddings.pt\")\n",
    "\n",
    "# similarities = []\n",
    "# for idx, tweet_emb in tqdm(enumerate(torch_tweet_embeddings), desc=\"Tweets\"):\n",
    "#     scores = defaultdict(float)\n",
    "    \n",
    "#     for chunk_idx, chunk_emb in tqdm(enumerate(torch_paper_chunk_embeddings), desc=\"Chunks\", leave=False):\n",
    "#         uid = paper_chunk_uids[chunk_idx]\n",
    "#         sim = cosine_similarity(tweet_emb.unsqueeze(0), chunk_emb.unsqueeze(0)).item()\n",
    "#         scores[uid] = max(scores[uid], sim)  # Keep best similarity per paper\n",
    "\n",
    "#     # Sort by similarity and get top_k papers\n",
    "#     top_papers = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "#     similarities.append({\n",
    "#         \"tweet\": dev_tweets[idx],\n",
    "#         \"gold_cord_uid\": dev_ids[idx],\n",
    "#         \"retrieved\": [uid for uid, score in top_papers]\n",
    "#     })\n",
    "\n",
    "# # Display a sample\n",
    "# for r in similarities[5:10]:\n",
    "#     print(f\"\\nTweet: {r['tweet']}\")\n",
    "#     print(f\"Gold paper: {r['gold_cord_uid']}\")\n",
    "#     print(\"Top 5 retrieved:\", r[\"retrieved\"])\n",
    "\n",
    "# def compute_mrr5(results):\n",
    "#     mrr = 0\n",
    "#     for result in results:\n",
    "#         gold_uid = result[\"gold_cord_uid\"]\n",
    "#         retrieved = result[\"retrieved\"]\n",
    "#         if gold_uid in retrieved:\n",
    "#             rank = retrieved.index(gold_uid) + 1\n",
    "#             mrr += 1 / rank\n",
    "#     return mrr / len(results)\n",
    "# mrr = compute_mrr5(similarities)\n",
    "# print(f\"Mean Reciprocal Rank (MRR@5): {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d60d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1584/1584 [00:26<00:00, 59.85it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 76.31it/s]\n",
      "Tweets: 1400it [1:37:12,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweet: IL-6 seems to be a primary catalyst of this uncontrolled inflammation in #covid19, and #tocilizumab, a mab IL-6 receptor blocker, has been used in small series of severe covid-19 cases with early reports of success\n",
      "Gold paper: 8cvjsisw\n",
      "Top 5 retrieved: ['z9jqbliw', '3r418rss', 'vx9vqr1k', 'zt5alyy2', 'xh723tgl']\n",
      "\n",
      "Tweet: macro-level, multi-national analysis shows that public mask compliance is negligible for stopping the spread of covid-19  â€” it appears to be socially damaging.\n",
      "Gold paper: tra5ewc5\n",
      "Top 5 retrieved: ['qi1henyy', '1s8jzzwg', 'zycgczqy', '763v4duh', 'jjh1z5c6']\n",
      "\n",
      "Tweet: Long, slender transmission chains of severe acute respiratory syndrome coronavirus (sars-cov-2) may go undetected for several weeks at low to moderate reproductive numbers: implications for containment and elimination strategy  [ðŸš¨preprint]\n",
      "Gold paper: yoiq6cgt\n",
      "Top 5 retrieved: ['yoiq6cgt', 'vmmztj0a', 'ueb7mjnv', 'hbkl5cam', 'w1azm2mc']\n",
      "\n",
      "Tweet: Significant vitamin D deficiency in people with COVID-19 was associated with a substantially higher mortality risk than COVID-19 patients with typical vitamin D levels (retrospective study in Bari, Italy).\n",
      "Gold paper: be8eu3qi\n",
      "Top 5 retrieved: ['u42d1p1j', 'qmo1zzkg', 'gg5c8v7d', 'vbnke2q5', 'md0drb25']\n",
      "\n",
      "Tweet: \"A strong connection is found between low serum zinc and serious cases of covid-19.\"\n",
      "Gold paper: c0ipfbeg\n",
      "Top 5 retrieved: ['c0ipfbeg', '4cnk76lb', 'wgbyqfrx', 'b0qj7tck', '2l5gc4x2']\n",
      "Mean Reciprocal Rank (MRR@5): 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import defaultdict\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def mini_embed_texts(texts, model, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        output = model.encode(batch, convert_to_tensor=True)\n",
    "        embeddings.extend(output)\n",
    "    return embeddings\n",
    "\n",
    "mini_paper_chunk_embeddings = mini_embed_texts(paper_chunk_texts, model)\n",
    "mini_tweet_embeddings = mini_embed_texts(dev_tweets, model)\n",
    "\n",
    "def compute_similarities(tweet_embeds, paper_embeds, paper_uids, top_k=5):\n",
    "    similarities = []\n",
    "    for idx, tweet_emb in tqdm(enumerate(tweet_embeds), desc=\"Tweets\"):\n",
    "        scores = defaultdict(float)\n",
    "        \n",
    "        for chunk_idx, chunk_emb in tqdm(enumerate(paper_embeds), desc=\"Chunks\", leave=False):\n",
    "            uid = paper_uids[chunk_idx]\n",
    "            sim = cosine_similarity(tweet_emb, chunk_emb, dim=0).item()\n",
    "            scores[uid] = max(scores[uid], sim)  # Keep best similarity per paper\n",
    "    \n",
    "        # Sort by similarity and get top_k papers\n",
    "        top_papers = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        similarities.append({\n",
    "            \"tweet\": dev_tweets[idx],\n",
    "            \"gold_cord_uid\": dev_ids[idx],\n",
    "            \"retrieved\": [uid for uid, score in top_papers]\n",
    "        })\n",
    "    return similarities\n",
    "\n",
    "similarities = compute_similarities(mini_tweet_embeddings, mini_paper_chunk_embeddings, paper_chunk_uids)\n",
    "\n",
    "# Display a sample\n",
    "for r in similarities[5:10]:\n",
    "    print(f\"\\nTweet: {r['tweet']}\")\n",
    "    print(f\"Gold paper: {r['gold_cord_uid']}\")\n",
    "    print(\"Top 5 retrieved:\", r[\"retrieved\"])\n",
    "\n",
    "def compute_mrr5(results):\n",
    "    mrr = 0\n",
    "    for result in results:\n",
    "        gold_uid = result[\"gold_cord_uid\"]\n",
    "        retrieved = result[\"retrieved\"]\n",
    "        if gold_uid in retrieved:\n",
    "            rank = retrieved.index(gold_uid) + 1\n",
    "            mrr += 1 / rank\n",
    "    return mrr / len(results)\n",
    "\n",
    "result = compute_mrr5(similarities) \n",
    "\n",
    "print(f\"Mean Reciprocal Rank (MRR@5): {result:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
